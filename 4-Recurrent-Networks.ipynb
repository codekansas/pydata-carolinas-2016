{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks use self-connections to model temporal sequences. A single \"recurrent layer\" takes input from its previous state as well as some other input. The image below is from the blog post [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy, which explains a lot about RNNs.\n",
    "\n",
    "![Recurrent Neural Network](images/karpathy_rnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all of the above architectures are easily modeled in Keras (in particular, the second and fourth are tricky to do, and require some workarounds). From the [Keras RNN documentation](https://keras.io/layers/recurrent/), we see that an RNN layer has input dimensions `(nb_samples, timesteps, input_dim)` and output dimensions of either `(nb_samples, timesteps, output_dim)` if the `return_sequences` argument is set to `True`, or `(nb_samples, output_dim)` otherwise. The first behavior corresponds to the fifth figure, while the second behavior corresponds to the third figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple RNN in Theano\n",
    "\n",
    "To understand how Keras works, it is a good idea to start off with something we can understand from the inside out. Let's build a vanilla recurrent neural network in Theano. The internal state of the network is updated according to the equation:\n",
    "\n",
    "```\n",
    "new_hidden_state = tanh(dot(input_vector, W) + dot(prev_hidden, U) + b)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `new_hidden_state` the updated state of the recurrent layer\n",
    "- `tanh` the activation function (the new state should be a nonlinear transformation of a linear combination of the new input step and its previous state)\n",
    "- `dot` the dot product between a vector and a matrix\n",
    "- `input_vector` the input at one timestep\n",
    "- `W` the input-to-hidden connection\n",
    "- `prev_hidden` the previous hidden state\n",
    "- `U` the hidden-to-hidden connection\n",
    "- `b` the bias vector (similar to a normal dense layer, it is a good idea to have a bias vector)\n",
    "\n",
    "As the number of variables in the equations gets bigger, it will become impractical to explain them all, but I've used a similar naming convention elsewhere in this notebook.\n",
    "\n",
    "First, let's import the necessary packages, define a random number generator and get the datatype to use for all our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "rng = np.random.RandomState(1337)\n",
    "dtype = theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make some helper methods for generating weights as shared variables, like we did with the XOR example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(n_in, n_out):\n",
    "    mag = 4. * np.sqrt(6. / (n_in + n_out))\n",
    "    W_value = np.asarray(rng.uniform(low=-mag, high=mag, size=(n_in, n_out)), dtype=dtype)\n",
    "    W = theano.shared(value=W_value, name='W_%d_%d' % (n_in, n_out), borrow=True, strict=False)\n",
    "    return W\n",
    "\n",
    "def get_bias(n_out):\n",
    "    b_value = np.asarray(np.zeros((n_out,), dtype=dtype), dtype=theano.config.floatX)\n",
    "    b = theano.shared(value=b_value, name='b_%d' % n_out, borrow=True, strict=False)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Theano XOR example, we used a simple subgradient descent optimizer. **Subgradient descent** means that, for a sample input, we change the weights of the network in the direction that minimizes the error between the desired output and the actual output. However, a good deal of work has gone into improving on this simple concept. Without getting into the details, a good optimizer for recurrent networks is the **RMSprop** optimizer. In Keras, it is very easy to specify that we want to use this optimizer, but since we are implementing everything in Theano right now, we have to define it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsprop(cost, params, learning_rate, rho=0.9, epsilon=1e-6):\n",
    "    updates = list()\n",
    "    for param in params:\n",
    "        accu = theano.shared(np.zeros(param.get_value(borrow=True).shape, dtype=dtype),\n",
    "                             broadcastable=param.broadcastable)\n",
    "        grad = T.grad(cost, param)\n",
    "        accu_new = rho * accu + (1 - rho) * grad ** 2\n",
    "\n",
    "        updates.append((accu, accu_new))\n",
    "        updates.append((param, param - (learning_rate * grad / T.sqrt(accu_new + epsilon))))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these helper methods, let's write a framework for testing how a general network performs. This framework takes as inputs the number of input and output dimensions, the input tensor of the network, and the predicted output, plus the network parameters (weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(n_in, n_out, X, output, params):\n",
    "    output = output[-1, :] # get the last timestep from the network\n",
    "    y = T.matrix(name='y', dtype=dtype) # the target variable\n",
    "    lr = T.scalar(name='lr', dtype=dtype) # the learning rate (as a variable we can change)\n",
    "\n",
    "    # minimize binary crossentropy\n",
    "    xent = -y * T.log(output) - (1 - y) * T.log(1 - output)\n",
    "    cost = xent.mean()\n",
    "    \n",
    "    # use rmsprop to get the network updates\n",
    "    updates = rmsprop(cost, params, lr)\n",
    "\n",
    "    # generate our testing data\n",
    "    t_sets = 10\n",
    "    X_datas = [np.asarray(rng.rand(20, n_in) > 0.5, dtype=dtype) for _ in range(t_sets)]\n",
    "    y_datas = [np.asarray(rng.rand(1, n_out) > 0.5, dtype=dtype) for _ in range(t_sets)]\n",
    "\n",
    "    # theano functions for training and testing\n",
    "    train = theano.function([X, y, lr], [cost], updates=updates)\n",
    "    test = theano.function([X], [output])\n",
    "\n",
    "    # some starting parameters\n",
    "    l = 0.1\n",
    "    n_train = 1000\n",
    "\n",
    "    # calculate and display the cost\n",
    "    cost = sum([train(X_data, y_data, 0)[0] for X_data, y_data in zip(X_datas, y_datas)])\n",
    "    print('Before training:', cost)\n",
    "\n",
    "    for i in range(n_train):\n",
    "        for X_data, y_data in zip(X_datas, y_datas):\n",
    "            train(X_data, y_data, l)\n",
    "\n",
    "        if (i+1) % (n_train / 5) == 0:\n",
    "            cost = sum([train(X_data, y_data, 0)[0] for X_data, y_data in zip(X_datas, y_datas)])\n",
    "            print('%d (lr = %f):' % (i+1, l), cost)\n",
    "            l *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these helper components, let's write the code for our basic RNN. This follows the equations seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 15.9748073443\n",
      "200 (lr = 0.100000): 13.5803246217\n",
      "400 (lr = 0.050000): 9.68473999735\n",
      "600 (lr = 0.025000): 7.76820204681\n",
      "800 (lr = 0.012500): 6.94260957224\n",
      "1000 (lr = 0.006250): 6.76449744031\n"
     ]
    }
   ],
   "source": [
    "def generate_and_test_vanilla_rnn(n_in, n_hidden, n_out):\n",
    "    X = T.matrix(name='X', dtype=dtype)\n",
    "\n",
    "    # the weights being used in the network\n",
    "    w_in = get_weights(n_in, n_hidden)\n",
    "    w_hidden = get_weights(n_hidden, n_hidden)\n",
    "    w_out = get_weights(n_hidden, n_out)\n",
    "\n",
    "    # the biases\n",
    "    b_hidden = get_bias(n_hidden)\n",
    "    b_out = get_bias(n_out)\n",
    "    h_0 = get_bias(n_hidden)\n",
    "    \n",
    "    # collect all the parameters here so we can pass them to the optimizer\n",
    "    params = [w_in, b_hidden, w_out, b_out, w_hidden, h_0]\n",
    "    \n",
    "    # define the recurrence here\n",
    "    def step(x_t, h_tm1):\n",
    "        h_t = T.tanh(T.dot(x_t, w_in) + T.dot(h_tm1, w_hidden) + b_hidden)\n",
    "        y_t = T.nnet.sigmoid(T.dot(h_t, w_out) + b_out)\n",
    "        return h_t, y_t\n",
    "\n",
    "    [_, output], _ = theano.scan(fn=step, sequences=X, outputs_info=[h_0, None], n_steps=X.shape[0])\n",
    "\n",
    "    test(n_in, n_out, X, output, params)\n",
    "\n",
    "generate_and_test_vanilla_rnn(10, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Gradient Decay\n",
    "\n",
    "The basic problem with simple RNNs is that they don't model long sequence very well. LSTMs and GRUs solve this problem. The graphic below was taken from [here](https://www.reddit.com/r/MachineLearning/comments/3vnwum/rnn_vs_lstm_vanishing_gradients/), and shows the magnitude of the gradients as we move backwards in time. As the graphic shows, the gradients of the basic RNN take only around 20 timesteps before they stop conveying information, while the LSTM gradients convey useful information even after 100 timesteps. In other words, **the effects of early inputs are much more pronounced in the LSTM network than the simple RNN network, so long time sequences are more easily differentiated**.\n",
    "\n",
    "![RNN Gradients Comparison](images/rnn_grads.gif)\n",
    "\n",
    "The LSTM network does this by using **gates**. Some of the information from the LSTM is siphoned into a separate state, where it sits around, unaffected by the network, before being let out later on. This is explained in good detail in [\"Understanding LSTM Networks\"](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah, which shows these graphics:\n",
    "\n",
    "### Basic RNN\n",
    "\n",
    "![Basic RNN](images/basic_rnn_graphic.png)\n",
    "\n",
    "### LSTM\n",
    "\n",
    "![LSTM](images/lstm_graphic.png)\n",
    "\n",
    "The equations defining an LSTM are as follow:\n",
    "\n",
    "```\n",
    "input_gate = tanh(dot(input_vector, W_input) + dot(prev_hidden, U_input) + b_input)\n",
    "forget_gate = tanh(dot(input_vector, W_forget) + dot(prev_hidden, U_forget) + b_forget)\n",
    "output_gate = tanh(dot(input_vector, W_output) + dot(prev_hidden, U_output) + b_output)\n",
    "\n",
    "candidate_state = tanh(dot(x, W_hidden) + dot(prev_hidden, U_hidden) + b_hidden)\n",
    "memory_unit = prev_candidate_state * forget_gate + candidate_state * input_gate\n",
    "\n",
    "new_hidden_state = tanh(memory_unit) * output_gate\n",
    "```\n",
    "\n",
    "It is a good idea to try to understand these equations at a reasonable level, because they play a role in the next notebook.\n",
    "\n",
    "Let's go ahead and implement this in Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 10.8128263049\n",
      "200 (lr = 0.100000): 1.94271907179e-05\n",
      "400 (lr = 0.050000): 1.16528041576e-05\n",
      "600 (lr = 0.025000): 9.67879741156e-06\n",
      "800 (lr = 0.012500): 8.92299780359e-06\n",
      "1000 (lr = 0.006250): 8.58776548077e-06\n"
     ]
    }
   ],
   "source": [
    "def generate_and_test_lstm(n_in, n_hidden, n_out):\n",
    "    X = T.matrix(name='X', dtype=dtype)\n",
    "\n",
    "    # there are a lot of parameters, so let's add them incrementally\n",
    "    params = list()\n",
    "\n",
    "    # input gate\n",
    "    w_in_input = get_weights(n_in, n_hidden)\n",
    "    w_hidden_input = get_weights(n_hidden, n_hidden)\n",
    "    b_input = get_bias(n_hidden)\n",
    "    params += [w_in_input, w_hidden_input, b_input]\n",
    "\n",
    "    # forget gate\n",
    "    w_in_forget = get_weights(n_in, n_hidden)\n",
    "    w_hidden_forget = get_weights(n_hidden, n_hidden)\n",
    "    b_forget = get_bias(n_hidden)\n",
    "    params += [w_in_forget, w_hidden_forget, b_forget]\n",
    "\n",
    "    # output gate\n",
    "    w_in_output = get_weights(n_in, n_hidden)\n",
    "    w_hidden_output = get_weights(n_hidden, n_hidden)\n",
    "    b_output = get_bias(n_hidden)\n",
    "    params += [w_in_output, w_hidden_output, b_output]\n",
    "\n",
    "    # hidden state\n",
    "    w_in_hidden = get_weights(n_in, n_hidden)\n",
    "    w_hidden_hidden = get_weights(n_hidden, n_hidden)\n",
    "    b_hidden = get_bias(n_hidden)\n",
    "    params += [w_in_hidden, w_hidden_hidden, b_hidden]\n",
    "\n",
    "    # output\n",
    "    w_out = get_weights(n_hidden, n_out)\n",
    "    b_out = get_bias(n_out)\n",
    "    params += [w_out, b_out]\n",
    "\n",
    "    # starting hidden and memory unit state\n",
    "    h_0 = get_bias(n_hidden)\n",
    "    c_0 = get_bias(n_hidden)\n",
    "    params += [h_0, c_0]\n",
    "    \n",
    "    # define the recurrence here\n",
    "    def step(x_t, h_tm1, c_tm1):\n",
    "        input_gate = T.nnet.sigmoid(T.dot(x_t, w_in_input) + T.dot(h_tm1, w_hidden_input) + b_input)\n",
    "        forget_gate = T.nnet.sigmoid(T.dot(x_t, w_in_forget) + T.dot(h_tm1, w_hidden_forget) + b_forget)\n",
    "        output_gate = T.nnet.sigmoid(T.dot(x_t, w_in_output) + T.dot(h_tm1, w_hidden_output) + b_output)\n",
    "\n",
    "        candidate_state = T.tanh(T.dot(x_t, w_in_hidden) + T.dot(h_tm1, w_hidden_hidden) + b_hidden)\n",
    "        memory_unit = c_tm1 * forget_gate + candidate_state * input_gate\n",
    "\n",
    "        h_t = T.tanh(memory_unit) * output_gate\n",
    "        y_t = T.nnet.sigmoid(T.dot(h_t, w_out) + b_out)\n",
    "\n",
    "        return h_t, memory_unit, y_t\n",
    "\n",
    "    [_, _, output], _ = theano.scan(fn=step, sequences=X, outputs_info=[h_0, c_0, None], n_steps=X.shape[0])\n",
    "\n",
    "    test(n_in, n_out, X, output, params)\n",
    "\n",
    "generate_and_test_lstm(10, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs, for good measure\n",
    "\n",
    "Since we've done LSTMs, we should do GRUs as well. Again from Christopher Olah's blog post, the GRU looks like:\n",
    "\n",
    "![GRU](images/gru_graphic.png)\n",
    "\n",
    "The equations are simpler than the LSTM equations, and look like:\n",
    "\n",
    "```\n",
    "update_gate = tanh(dot(input_vector, W_update) + dot(prev_hidden, U_update) + b_update)\n",
    "reset_gate = tanh(dot(input_vector, W_reset) + dot(prev_hidden, U_reset) + b_reset)\n",
    "\n",
    "reset_hidden = prev_hidden * reset_gate\n",
    "temp_state = tanh(dot(input_vector, W_hidden) + dot(reset_hidden, U_reset) + b_hidden)\n",
    "new_hidden_state = (1 - update_gate) * temp_state + update_gate * prev_hidden\n",
    "```\n",
    "\n",
    "Let's go ahead and implement this in Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 2.49044999098\n",
      "200 (lr = 0.100000): 5.0814619226e-06\n",
      "400 (lr = 0.050000): 3.28566008242e-06\n",
      "600 (lr = 0.025000): 2.79025599715e-06\n",
      "800 (lr = 0.012500): 2.59417142659e-06\n",
      "1000 (lr = 0.006250): 2.5060001764e-06\n"
     ]
    }
   ],
   "source": [
    "def generate_and_test_gru(n_in, n_hidden, n_out):\n",
    "    X = T.matrix(name='X', dtype=dtype)\n",
    "\n",
    "    # there are a lot of parameters, so let's add them incrementally\n",
    "    params = list()\n",
    "\n",
    "    # update gate\n",
    "    w_in_update = get_weights(n_in, n_hidden)\n",
    "    w_hidden_update = get_weights(n_hidden, n_hidden)\n",
    "    b_update = get_bias(n_hidden)\n",
    "    params += [w_in_update, w_hidden_update, b_update]\n",
    "\n",
    "    # reset gate\n",
    "    w_in_reset = get_weights(n_in, n_hidden)\n",
    "    w_hidden_reset = get_weights(n_hidden, n_hidden)\n",
    "    b_reset = get_bias(n_hidden)\n",
    "    params += [w_in_reset, w_hidden_reset, b_reset]\n",
    "\n",
    "    # hidden layer\n",
    "    w_in_hidden = get_weights(n_in, n_hidden)\n",
    "    w_reset_hidden = get_weights(n_hidden, n_hidden)\n",
    "    b_in_hidden = get_bias(n_hidden)\n",
    "    params += [w_in_hidden, w_reset_hidden, b_in_hidden]\n",
    "\n",
    "    # output\n",
    "    w_out = get_weights(n_hidden, n_out)\n",
    "    b_out = get_bias(n_out)\n",
    "    params += [w_out, b_out]\n",
    "\n",
    "    # starting hidden state\n",
    "    h_0 = get_bias(n_hidden)\n",
    "    params += [h_0]\n",
    "    \n",
    "    # define the recurrence here\n",
    "    def step(x_t, h_tm1):\n",
    "        update_gate = T.nnet.sigmoid(T.dot(x_t, w_in_update) + T.dot(h_tm1, w_hidden_update) + b_update)\n",
    "        reset_gate = T.nnet.sigmoid(T.dot(x_t, w_in_reset) + T.dot(h_tm1, w_hidden_reset) + b_reset)\n",
    "        h_t_temp = T.tanh(T.dot(x_t, w_in_hidden) + T.dot(h_tm1 * reset_gate, w_reset_hidden) + b_in_hidden)\n",
    "\n",
    "        h_t = (1 - update_gate) * h_t_temp + update_gate * h_tm1\n",
    "        y_t = T.nnet.sigmoid(T.dot(h_t, w_out) + b_out)\n",
    "\n",
    "        return h_t, y_t\n",
    "\n",
    "    [_, output], _ = theano.scan(fn=step, sequences=X, outputs_info=[h_0, None], n_steps=X.shape[0])\n",
    "\n",
    "    test(n_in, n_out, X, output, params)\n",
    "\n",
    "generate_and_test_gru(10, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making this a whole lot less painful with Keras\n",
    "\n",
    "One of the points of going through all these equations was to emphasize how much of a pain it is to do this. We didn't even try stacking layers together! Let's do the exact same thing we did above using Keras, including using an RMSprop optimizer and binary crossentropy cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- SimpleRNN --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before: 4.78796672821\n",
      "Error after: 0.805904984474\n",
      "-- LSTM --\n",
      "Error before: 5.31922245026\n",
      "Error after: 3.22388005257\n",
      "-- GRU --\n",
      "Error before: 3.71665763855\n",
      "Error after: 1.19209303762e-07\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "input_dims, output_dims = 10, 2\n",
    "sequence_length = 20\n",
    "n_test = 10\n",
    "\n",
    "# generate some random data to train on\n",
    "X_data = np.asarray([np.asarray(rng.rand(20, input_dims) > 0.5, dtype=dtype) for _ in range(n_test)])\n",
    "y_data = np.asarray([np.asarray(rng.rand(output_dims,) > 0.5, dtype=dtype) for _ in range(n_test)])\n",
    "\n",
    "# put together rnn models\n",
    "from keras.layers import Input\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "\n",
    "input_sequence = Input(shape=(sequence_length, input_dims,), dtype='float32')\n",
    "\n",
    "# this is so much easier!\n",
    "vanilla = SimpleRNN(output_dims, return_sequences=False)\n",
    "lstm = LSTM(output_dims, return_sequences=False)\n",
    "gru = GRU(output_dims, return_sequences=False)\n",
    "rnns = [vanilla, lstm, gru]\n",
    "\n",
    "# train the models\n",
    "for rnn in rnns:\n",
    "    model = Model(input=[input_sequence], output=rnn(input_sequence))\n",
    "    model.compile(optimizer=RMSprop(lr=0.1), loss='binary_crossentropy')\n",
    "    print('-- %s --' % rnn.__class__.__name__)\n",
    "    print('Error before: {}'.format(model.evaluate([X_data], [y_data], verbose=0)))\n",
    "    model.fit([X_data], [y_data], nb_epoch=1000, verbose=0)\n",
    "    print('Error after: {}'.format(model.evaluate([X_data], [y_data], verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
